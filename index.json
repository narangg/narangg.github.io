[{"authors":["admin"],"categories":null,"content":"I am a PhD student in the School of Electrical Engineering and Computer Science at Washington State University.\nMy general research interests are in artificial intelligence (AI) and machine learning (ML) with a focus on probabilistic modeling and optimization to support decision-making under uncertainty in structured domains. The overarching goal of my research is to develop principled AI and ML solutions to accelerate engineering design and scientific discovery towards high-impact sustainability applications. My current research is motivated from real-world applications such as computing systems design and resource management; design of nanoporous materials for gas storage (e.g., hydrogen powered cars), gas separation (e.g., capturing carbon dioxide), and gas sensing (e.g., detecting toxic compounds and explosives); and design of microbiomes for human health and agriculture applications. Specific topics include:  Learning probabilistic models over structured data Methods to combine domain knowledge and experimental data to create rich models Knowledge representation and learning for tractable reasoning Sequential decision-making under uncertainty for data collection  Before grad school, I got my Bachelors in Technology (B.Tech.) degree in Mathematics and Computing from Delhi Technological University, New Delhi.\nI am very fortunate to work with my advisor Dr. Jana Doppa.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://aryandeshwal.github.io/author/aryan-deshwal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/aryan-deshwal/","section":"authors","summary":"I am a PhD student in the School of Electrical Engineering and Computer Science at Washington State University.\nMy general research interests are in artificial intelligence (AI) and machine learning (ML) with a focus on probabilistic modeling and optimization to support decision-making under uncertainty in structured domains.","tags":null,"title":"Aryan Deshwal","type":"authors"},{"authors":["Aryan Deshwal","Sebastian Ament","Maximilian Balandat","Eytan Bakshy","Janardhan Rao Doppa","David Eriksson"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662693568,"objectID":"087784f000f78256e3e88a52b5416b18","permalink":"https://aryandeshwal.github.io/publication/deshwal-2023-aistats/","publishdate":"2022-09-22T03:19:28.396322Z","relpermalink":"/publication/deshwal-2023-aistats/","section":"publication","summary":"We consider the problem of optimizing expensive black-box functions over high-dimensional combinatorial spaces which arises in many science, engineering, and ML applications. We use Bayesian Optimization (BO) and propose a novel surrogate modeling approach for efficiently handling a large number of binary and categorical parameters. The key idea is to select a number of discrete structures from the input space (the dictionary) and use them to define an ordinal embedding for high-dimensional combinatorial structures. This allows us to use existing Gaussian process models for continuous spaces. We develop a principled approach based on binary wavelets to construct dictionaries for binary spaces, and propose a randomized construction method that generalizes to categorical spaces. We provide theoretical justification to support the effectiveness of the dictionary-based embeddings. Our experiments on diverse real-world benchmarks demonstrate the effectiveness of our proposed surrogate modeling approach over state-of-the-art BO methods.","tags":[],"title":"Bayesian Optimization over High-Dimensional Combinatorial Spaces via Dictionary-based Embeddings","type":"publication"},{"authors":["Aryan Deshwal","Syrine Belakaria","Janardhan Rao Doppa","Dae Hyun Kim"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631157568,"objectID":"5e1ac7af9a6724773ff50e0a994054b3","permalink":"https://aryandeshwal.github.io/publication/deshwal-2022-aaai/","publishdate":"2021-09-22T03:19:28.396322Z","relpermalink":"/publication/deshwal-2022-aaai/","section":"publication","summary":"Optimizing expensive to evaluate black-box functions over an input space consisting of all permutations of d objects is an important problem with many real-world applications. For example, placement of functional blocks in hardware design to optimize performance via simulations. The overall goal is to minimize the number of function evaluations to find high-performing permutations. The key challenge in solving this problem using the Bayesian optimization (BO) framework is to trade-off the complexity of statistical model and tractability of acquisition function optimization. In this paper, we propose and evaluate two algorithms for BO over Permutation Spaces (BOPS). First, BOPS-T employs Gaussian process (GP) surrogate model with Kendall kernels and a Tractable acquisition function optimization approach based on Thompson sampling to select the sequence of permutations for evaluation. Second, BOPS-H employs GP surrogate model with Mallow kernels and a Heuristic search approach to optimize expected improvement acquisition function. We theoretically analyze the performance of BOPS-T to show that their regret grows sub-linearly. Our experiments on multiple synthetic and real-world benchmarks show that both BOPS-T and BOPS-H perform better than the state-of-the-art BO algorithm for combinatorial spaces. To drive future research on this important problem, we make new resources and real-world benchmarks available to the community.","tags":[],"title":"Bayesian Optimization over Permutation Spaces","type":"publication"},{"authors":["Aryan Deshwal","Janardhan Rao Doppa"],"categories":[],"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600744768,"objectID":"89afcfa46f2a929fbbdffe67daab738e","permalink":"https://aryandeshwal.github.io/publication/deshwal-2021-neurips/","publishdate":"2021-10-22T03:19:28.396322Z","relpermalink":"/publication/deshwal-2021-neurips/","section":"publication","summary":"We consider the problem of optimizing combinatorial spaces (e.g., sequences, trees, and graphs) using expensive black-box function evaluations. For example, optimizing molecules for drug design using physical lab experiments. Bayesian optimization (BO) is an efficient framework for solving such problems by intelligently selecting the inputs with high utility guided by a learned surrogate model. A recent BO approach for combinatorial spaces is through a reduction to BO over continuous spaces by learning a latent representation of structures using deep generative models (DGMs). The selected input from the continuous space is decoded into a discrete structure for performing function evaluation. However, the surrogate model over the latent space only uses the information learned by the DGM, which may not have the desired inductive bias to approximate the target black-box function. To overcome this drawback, this paper proposes a principled approach referred as LADDER. The key idea is to define a novel structure-coupled kernel that explicitly integrates the structural information from decoded structures with the learned latent space representation for better surrogate modeling. Our experiments on real-world benchmarks show that LADDER significantly improves over the BO over latent space method, and performs better or similar to state-of-the-art methods.","tags":[],"title":"Combining Latent Space and Structured Kernels for Bayesian Optimization over Combinatorial Spaces","type":"publication"},{"authors":["Aryan Deshwal","Cory Simon","Janardhan Rao Doppa"],"categories":[],"content":"","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631157568,"objectID":"72b79ec3e155926d8cc8685daabcbde1","permalink":"https://aryandeshwal.github.io/publication/deshwal-2021-mofs/","publishdate":"2021-09-22T03:19:28.396322Z","relpermalink":"/publication/deshwal-2021-mofs/","section":"publication","summary":"Nanoporous materials (NPMs) could be used to store, capture, and sense many different gases. Given an adsorption task, we often wish to search a library of NPMs for the one with the optimal adsorption property. The high cost of NPM synthesis and gas adsorption measurements, whether these experiments are in the lab or in a simulation, often precludes exhaustive search. We explain, demonstrate, and advocate Bayesian optimization (BO) to actively search for the optimal NPM in a library of NPMs-- and find it using the fewest experiments. The two ingredients of BO are a surrogate model and an acquisition function. The surrogate model is a probabilistic model reflecting our beliefs about the NPM-structure--property relationship based on observations from past experiments. The acquisition function uses the surrogate model to score each NPM according to the utility of picking it for the next experiment. It balances two competing goals: (a) exploitation of our current approximation of the structure-property relationship to pick the highest-performing NPM, and (b) exploration of blind spots in the NPM space to pick an NPM we are uncertain about, to improve our approximation of the structure-property relationship. We demonstrate BO by searching an open database of ~70,000 hypothetical covalent organic frameworks (COFs) for the COF with the highest simulated methane deliverable capacity. BO finds the optimal COF and acquires 30% of the top 100 highest-ranked COFs after evaluating only ~120 COFs. More, BO searches more efficiently than evolutionary and one-shot supervised machine learning approaches.","tags":[],"title":"Bayesian Optimization of Nanoporous Materials","type":"publication"},{"authors":["Syrine Belakaria","Aryan Deshwal","Janardhan Rao Doppa"],"categories":[],"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600744767,"objectID":"2466195aac1dc51ad7b0b4167e878f19","permalink":"https://aryandeshwal.github.io/publication/belakaria-2021-journal/","publishdate":"2021-09-22T03:19:27.654687Z","relpermalink":"/publication/belakaria-2021-journal/","section":"publication","summary":"We consider the problem of black-box multi-objective optimization (MOO) using expensive function evaluations (also referred to as experiments), where the goal is to approximate the true Pareto set of solutions by minimizing the total resource cost of experiments. For example, in hardware design optimization, we need to find the designs that trade-off performance, energy, and area overhead using expensive computational simulations. The key challenge is to select the sequence of experiments to uncover high-quality solutions using minimal resources. In this paper, we propose a general framework for solving MOO problems based on the principle of output space entropy (OSE) search: select the experiment that maximizes the information gained per unit resource cost about the true Pareto front. We appropriately instantiate the principle of OSE search to derive efficient algorithms for the following four MOO problem settings: 1) The most basic em single-fidelity setting, where experiments are expensive and accurate; 2) Handling em black-box constraints} which cannot be evaluated without performing experiments; 3) The discrete multi-fidelity setting, where experiments can vary in the amount of resources consumed and their evaluation accuracy; and 4) The em continuous-fidelity setting, where continuous function approximations result in a huge space of experiments. Experiments on diverse synthetic and real-world benchmarks show that our OSE search based algorithms improve over state-of-the-art methods in terms of both computational-efficiency and accuracy of MOO solutions.","tags":[],"title":"Output Space Entropy Search Framework for Multi-Objective Bayesian Optimization","type":"publication"},{"authors":["Aryan Deshwal","Syrine Belakaria","Janardhan Rao Doppa"],"categories":[],"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623208768,"objectID":"02a33ce1e993c9713fd278335b2eb4dd","permalink":"https://aryandeshwal.github.io/publication/deshwal-2021-icml/","publishdate":"2020-09-22T03:19:28.396322Z","relpermalink":"/publication/deshwal-2021-icml/","section":"publication","summary":"We consider the problem of optimizing hybrid structures (mixture of discrete and continuous input variables) via expensive black-box function evaluations. This problem arises in many real-world applications. For example, in materials design optimization via lab experiments, discrete and continuous variables correspond to the presence/absence of primitive elements and their relative concentrations respectively. The key challenge is to accurately model the complex interactions between discrete and continuous variables. In this paper, we propose a novel approach referred as Hybrid Bayesian Optimization (HyBO) by utilizing diffusion kernels, which are naturally defined over continuous and discrete variables. We develop a principled approach for constructing diffusion kernels over hybrid spaces by utilizing the additive kernel formulation, which allows additive interactions of all orders in a tractable manner. We theoretically analyze the modeling strength of additive hybrid kernels and prove that it has the universal approximation property. Our experiments on synthetic and six diverse real-world benchmarks show that HyBO significantly outperforms the state-of-the-art methods.","tags":[],"title":"Bayesian Optimization over Hybrid Spaces","type":"publication"},{"authors":["Biresh Kumar Joardar","Aryan Deshwal","Janardhan Rao Doppa","Partha Pratim Pande","Krishnendu Chakrabarty"],"categories":[],"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620530368,"objectID":"0704c99ab051c394785f31a88ae5e541","permalink":"https://aryandeshwal.github.io/publication/joardar-2021-tcad/","publishdate":"2020-09-22T03:19:28.396322Z","relpermalink":"/publication/joardar-2021-tcad/","section":"publication","summary":"Resistive random-access memory (ReRAM)-based architectures can be used to accelerate Convolutional Neural Network (CNN) training. However, existing architectures either do not support normalization at all or they support only a limited version of it. Moreover, it is common practice for CNNs to add normalization layers after every convolution layer. In this work, we show that while normalization layers are necessary to train deep CNNs, only a few such layers are sufficient for effective training. A large number of normalization layers do not improve prediction accuracy; it necessitates additional hardware and gives rise to performance bottlenecks. To address this problem, we propose DeepTrain, a heterogeneous architecture enabled by a Bayesian optimization (BO) methodology; together, they provide adequate hardware and software support for normalization operations. The proposed BO methodology determines the minimum number of normalization operations necessary for a given CNN. Experimental evaluation indicates that the BO-enabled DeepTrain architecture achieves up to 15X speed-up compared to a conventional GPU for training CNNs with no accuracy loss while utilizing only a few normalization layers.","tags":[],"title":"High-Throughput Training of Deep CNNs on ReRAM-based Heterogeneous Architectures via Optimized Normalization Layers","type":"publication"},{"authors":["Aryan Deshwal","Syrine Belakaria","Ganapati Bhat","Janardhan Rao Doppa","Partha Pratim Pande"],"categories":[],"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620530368,"objectID":"c3c42f86b4636a106bd8bfc93c9d69bc","permalink":"https://aryandeshwal.github.io/publication/deshwal-2021-dac/","publishdate":"2020-09-22T03:19:28.396322Z","relpermalink":"/publication/deshwal-2021-dac/","section":"publication","summary":"Mobile system-on-chips (SoCs) are growing in their complexity and heterogeneity (e.g., Arm Big-Little architecture) to meet the needs of emerging applications, including games and artificial intelligence. This makes it very challenging to optimally manage the resources (e.g., controlling the number and frequency of different types of cores) at runtime to meet the desired trade-offs among multiple objectives such as performance and energy. This paper proposes a novel information-theoretic framework referred to as PaRMIS to create Pareto-optimal resource management policies for given target applications and design objectives. PaRMIS specifies parametric policies to manage resources and learns statistical models from candidate policy evaluation data in the form of target design objective values. The key idea is to select a candidate policy for evaluation in each iteration guided by statistical models that maximize the information gain about the true Pareto front. Experiments on a commercial heterogeneous SoC show that PaRMIS achieves better Pareto fronts and is easily usable to optimize complex objectives (e.g., performance per Watt) when compared to prior methods.","tags":[],"title":"Learning Pareto-Frontier Resource Management Policies for Heterogeneous SoCs - An Information-Theoretic Approach","type":"publication"},{"authors":["Aryan Deshwal","Syrine Belakaria","Janardhan Rao Doppa"],"categories":[],"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600744768,"objectID":"af3d3f542c5f33cca6ac2a26d89a758a","permalink":"https://aryandeshwal.github.io/publication/deshwal-2021-aaai/","publishdate":"2020-09-22T03:19:28.396322Z","relpermalink":"/publication/deshwal-2021-aaai/","section":"publication","summary":"Bayesian optimization (BO) is an efficient framework for solving black-box optimization problems with expensive function evaluations. This paper addresses the BO problem setting for combinatorial spaces (e.g., sequences and graphs) that occurs naturally in science and engineering applications. A prototypical example is molecular optimization guided by expensive experiments. The key challenge is to balance the complexity of statistical models and tractability of search to select combinatorial structures for evaluation. In this paper, we propose an efficient approach referred as Mercer Features for Combinatorial Bayesian Optimization (MerCBO). The key idea behind MerCBO is to provide explicit feature maps for diffusion kernels over discrete objects by exploiting the structure of their combinatorial graph representation. These Mercer features combined with Thompson sampling as the acquisition function allows us to employ efficient solvers for finding the next structure for evaluation. Experimental evaluation on diverse real-world benchmarks demonstrates that MerCBO performs similarly or better than prior methods.","tags":[],"title":"Mercer Features for Efficient Combinatorial Bayesian Optimization","type":"publication"},{"authors":["Syrine Belakaria","Aryan Deshwal","Janardhan Rao Doppa"],"categories":[],"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600744768,"objectID":"359743054050ff7671318778ccdf7813","permalink":"https://aryandeshwal.github.io/publication/belakaria-2020-imoca/","publishdate":"2020-09-22T03:19:28.544737Z","relpermalink":"/publication/belakaria-2020-imoca/","section":"publication","summary":"Many real-world applications involve black-box optimization of multiple objectives using continuous function approximations that trade-off accuracy and resource cost of evaluation. For example, in rocket launching research, we need to find designs that trade-off return-time and angular distance using continuous-fidelity simulators (eg, varying tolerance parameter to trade-off simulation time and accuracy) for design evaluations. The goal is to approximate the optimal Pareto set by minimizing the cost for evaluations. In this paper, we propose a novel approach referred to as information-Theoretic Multi-Objective Bayesian Optimization with Continuous Approximations (iMOCA)} to solve this problem. The key idea is to select the sequence of input and function approximations for multiple objectives which maximize the information gain per unit cost for the optimal Pareto front. Our experiments on diverse synthetic and real-world benchmarks show that iMOCA significantly improves over existing single-fidelity methods.","tags":[],"title":"Information-Theoretic Multi-Objective Bayesian Optimization with Continuous Approximations","type":"publication"},{"authors":["Syrine Belakaria","Aryan Deshwal","Janardhan Rao Doppa"],"categories":[],"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600744768,"objectID":"295333d5728daa4eee65a2976139db9d","permalink":"https://aryandeshwal.github.io/publication/belakaria-2020-max/","publishdate":"2020-09-22T03:19:28.544737Z","relpermalink":"/publication/belakaria-2020-max/","section":"publication","summary":"We consider the problem of constrained multi-objective blackbox optimization using expensive function evaluations, where the goal is to approximate the true Pareto set of solutions satisfying a set of constraints while minimizing the number of function evaluations. For example, in aviation power system design applications, we need to find the designs that trade-off total energy and the mass while satisfying specific thresholds for motor temperature and voltage of cells. This optimization requires performing expensive computational simulations to evaluate designs. In this paper, we propose a new approach referred as {\\em Max-value Entropy Search for Multi-objective Optimization with Constraints (MESMOC)} to solve this problem. MESMOC employs an output-space entropy based acquisition function to efficiently select the sequence of inputs for evaluation to uncover high-quality pareto-set solutions while satisfying constraints.","tags":[],"title":"Max-value Entropy Search for Multi-Objective Bayesian Optimization with Constraints","type":"publication"},{"authors":["Aryan Deshwal","Syrine Belakaria","Janardhan Rao Doppa"],"categories":[],"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600744768,"objectID":"0e9cc8055f0bf41b2a977cb2b7d665ec","permalink":"https://aryandeshwal.github.io/publication/deshwal-2020-scalable/","publishdate":"2020-09-22T03:19:28.396322Z","relpermalink":"/publication/deshwal-2020-scalable/","section":"publication","summary":"We study the problem of optimizing expensive blackbox functions over combinatorial spaces (eg, sets, sequences, trees, and graphs). BOCS (Baptista and Poloczek, 2018) is a state-of-the-art Bayesian optimization method for tractable statistical models, which performs semi-definite programming based acquisition function optimization (AFO) to select the next structure for evaluation. Unfortunately, BOCS scales poorly for large number of binary and/or categorical variables. Based on recent advances in submodular relaxation (Ito and Fujimaki, 2016) for solving Binary Quadratic Programs, we study an approach referred as Parametrized Submodular Relaxation (PSR) towards the goal of improving the scalability and accuracy of solving AFO problems for BOCS model. PSR approach relies on two key ideas. First, reformulation of AFO problem as submodular relaxation with some unknown parameters, which can be solved efficiently using minimum graph cut algorithms. Second, construction of an optimization problem to estimate the unknown parameters with close approximation to the true objective. Experiments on diverse benchmark problems show significant improvements with PSR for BOCS model.","tags":[],"title":"Scalable Combinatorial Bayesian Optimization with Tractable Statistical models","type":"publication"},{"authors":["Aryan Deshwal","Syrine Belakaria","Janardhan Rao Doppa","Alan Fern"],"categories":[],"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600744767,"objectID":"88610b44a62fef0adca546957ddbb222","permalink":"https://aryandeshwal.github.io/publication/deshwal-2020-optimizing/","publishdate":"2020-09-22T03:19:27.051221Z","relpermalink":"/publication/deshwal-2020-optimizing/","section":"publication","summary":"We consider the problem of optimizing expensive black-box functions over discrete spaces (e.g., sets, sequences, graphs). The key challenge is to select a sequence of combinatorial structures to evaluate, in order to identify high-performing structures as quickly as possible. Our main contribution is to introduce and evaluate a new learning-to-search framework for this problem called L2S-DISCO. The key insight is to employ search procedures guided by control knowledge at each step to select the next structure and to improve the control knowledge as new function evaluations are observed. We provide a concrete instantiation of L2S-DISCO for local search procedure and empirically evaluate it on diverse real-world benchmarks. Results show the efficacy of L2S-DISCO over state-of-the-art algorithms in solving complex optimization problems.","tags":[],"title":"Optimizing Discrete Spaces via Expensive Evaluations: A Learning to Search Framework","type":"publication"},{"authors":["Syrine Belakaria","Aryan Deshwal","Nitthilan Kannappan Jayakodi","Janardhan Rao Doppa"],"categories":[],"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600744767,"objectID":"eda852a119c77f125376cbdaf593eb62","permalink":"https://aryandeshwal.github.io/publication/belakaria-2020-uncertainty/","publishdate":"2020-09-22T03:19:26.897879Z","relpermalink":"/publication/belakaria-2020-uncertainty/","section":"publication","summary":"We consider the problem of multi-objective (MO) blackbox optimization using expensive function evaluations, where the goal is to approximate the true Pareto set of solutions while minimizing the number of function evaluations. For example, in hardware design optimization, we need to find the designs that trade-off performance, energy, and area overhead using expensive simulations. We propose a novel uncertainty-aware search framework referred to as USeMO to efficiently select the sequence of inputs for evaluation to solve this problem. The selection method of USeMO consists of solving a cheap MO optimization problem via surrogate models of the true functions to identify the most promising candidates and picking the best candidate based on a measure of uncertainty. We also provide theoretical analysis to characterize the efficacy of our approach. Our experiments on several synthetic and six diverse real-world benchmark problems show that USeMO consistently outperforms the state-of-the-art algorithms.","tags":[],"title":"Uncertainty-Aware Search Framework for Multi-Objective Bayesian Optimization","type":"publication"},{"authors":["Nitthilan Kanappan Jayakodi","Syrine Belakaria","Aryan Deshwal","Janardhan Rao Doppa"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600744767,"objectID":"adb1692a9c85784bf5062f8cd67085df","permalink":"https://aryandeshwal.github.io/publication/jayakodi-2020-design/","publishdate":"2020-09-22T03:19:27.353414Z","relpermalink":"/publication/jayakodi-2020-design/","section":"publication","summary":"Many real-world edge applications including object detection, robotics, and smart health are enabled by deploying deep neural networks (DNNs) on energy-constrained mobile platforms. In this article, we propose a novel approach to trade off energy and accuracy of inference at runtime using a design space called Learning Energy Accuracy Tradeoff Networks (LEANets). The key idea behind LEANets is to design classifiers of increasing complexity using pretrained DNNs to perform input-specific adaptive inference. The accuracy and energy consumption of the adaptive inference scheme depends on a set of thresholds, one for each classifier. To determine the set of threshold vectors to achieve different energy and accuracy tradeoffs, we propose a novel multiobjective optimization approach. We can select the appropriate threshold vector at runtime based on the desired tradeoff. We perform experiments on multiple pretrained DNNs including ConvNet, VGG-16, and MobileNet using diverse image classification datasets. Our results show that we get up to a 50% gain in energy for negligible loss in accuracy, and optimized LEANets achieve significantly better energy and accuracy tradeoff when compared to a state-of-the-art method referred to as Slimmable neural networks.","tags":[],"title":"Design and Optimization of Energy-Accuracy Tradeoff Networks for Mobile Platforms via Pretrained Deep Models","type":"publication"},{"authors":["Zhiyuan Zhou","Syrine Belakaria","Aryan Deshwal","Wookpyo Hong","Janardhan Rao Doppa","Partha Pratim Pande","Deukhyoun Heo"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600744768,"objectID":"fd224535a9c80060fe0c8d5ce6fa0083","permalink":"https://aryandeshwal.github.io/publication/zhou-2020-design/","publishdate":"2020-09-22T03:19:27.953852Z","relpermalink":"/publication/zhou-2020-design/","section":"publication","summary":"Efficiency of power management system (PMS) is one of the key performance metrics for highly integrated system on chips (SoCs). Towards the goal of improving power efficiency of SoCs, we make two key technical contributions in this paper. First, we develop a multi-output switched-capacitor voltage regulator (SCVR) with a new flying capacitor crossing technique (FCCT) and cloud-capacitor method. Second, to optimize the design parameters of SCVR, we introduce a novel machine¬learning (ML)-inspired optimization framework to reduce the number of expensive design simulations. Simulation shows that power loss of the multi-output SCVR with FCCT is reduced by more than 40% compared to conventional multiple single-output SCVRs. Our ML-based design optimization framework is able to achieve more than 90% reduction in the number of simulations needed to uncover optimized circuit parameters of the proposed SCVR.","tags":[],"title":"Design of Multi-Output Switched-Capacitor Voltage Regulator via Machine Learning","type":"publication"},{"authors":["Syrine Belakaria","Aryan Deshwal","Janardhan Rao Doppa"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600744767,"objectID":"182464307b30921802ca781af344ea84","permalink":"https://aryandeshwal.github.io/publication/belakaria-2020-mf-osemo/","publishdate":"2020-09-22T03:19:26.897879Z","relpermalink":"/publication/belakaria-2020-mf-osemo/","section":"publication","summary":"We study the novel problem of blackbox optimization of multiple objectives via multi-fidelity function evaluations that vary in the amount of resources consumed and their accuracy. The overall goal is to approximate the true Pareto set of solutions by minimizing the resources consumed for function evaluations. For example, in power system design optimization, we need to find designs that trade-off cost, size, efficiency, and thermal tolerance using multi-fidelity simulators for design evaluations. In this paper, we propose a novel approach referred as Multi-Fidelity Output Space Entropy Search for Multi-objective Optimization (MF-OSEMO) to solve this problem. The key idea is to select the sequence of candidate input and fidelity-vector pairs that maximize the information gained about the true Pareto front per unit resource cost. Our experiments on several synthetic and real-world benchmark problems show that MF-OSEMO, with both approximations, significantly improves over the state-of-the-art single-fidelity algorithms for multi-objective optimization.","tags":[],"title":"Multi-Fidelity Multi-Objective Bayesian Optimization, An Output Space Entropy Search Approach","type":"publication"},{"authors":["Syrine Belakaria","Aryan Deshwal","Janardhan Rao Doppa"],"categories":[],"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600744767,"objectID":"0290f2246eae4bfabca526ce064c5075","permalink":"https://aryandeshwal.github.io/publication/belakaria-2019-max/","publishdate":"2020-09-22T03:19:27.654687Z","relpermalink":"/publication/belakaria-2019-max/","section":"publication","summary":"We consider the problem of multi-objective (MO) blackbox optimization using expensive function evaluations, where the goal is to approximate the true Pareto-set of solutions by minimizing the number of function evaluations. For example, in hardware design optimization, we need to find the designs that trade-off performance, energy, and area overhead using expensive simulations. We propose a novel approach referred to as Max-value Entropy Search for Multi-objective Optimization (MESMO) to solve this problem. MESMO employs an output-space entropy based acquisition function to efficiently select the sequence of inputs for evaluation for quickly uncovering high-quality solutions. We also provide theoretical analysis to characterize the efficacy of MESMO. Our experiments on several synthetic and real-world benchmark problems show that MESMO consistently outperforms state-of-the-art algorithms.","tags":[],"title":"Max-value Entropy Search for Multi-Objective Bayesian Optimization","type":"publication"},{"authors":["Aryan Deshwal","Nitthilan Kanappan Jayakodi","Biresh Kumar Joardar","Janardhan Rao Doppa","Partha Pratim Pande"],"categories":[],"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600744767,"objectID":"51d35b8323c435d311a7763d844a52d0","permalink":"https://aryandeshwal.github.io/publication/deshwal-2019-moos/","publishdate":"2020-09-22T03:19:27.507104Z","relpermalink":"/publication/deshwal-2019-moos/","section":"publication","summary":"The growing needs of emerging applications has posed significant challenges for the design of optimized manycore systems. Network-on-Chip (NoC) enables the integration of a large number of processing elements (PEs) in a single die. To design optimized manycore systems, we need to establish suitable trade-offs among multiple objectives including power, performance, and thermal. Therefore, we consider multi-objective design space exploration (MO-DSE) problems arising in the design of NoC-enabled manycore systems: placement of PEs and communication links to optimize two or more objectives (e.g., latency, energy, and throughput). Existing algorithms to solve MO-DSE problems suffer from scalability and accuracy challenges as size of the design space and the number of objectives grow. In this paper, we propose a novel framework referred as Multi-Objective Optimistic Search (MOOS) that performs adaptive design space exploration using a data-driven model to improve the speed and accuracy of multi-objective design optimization process. We apply MOOS to design both 3D heterogeneous and homogeneous manycore systems using Rodinia, PARSEC, and SPLASH2 benchmark suites. We demonstrate that MOOS improves the speed of finding solutions compared to state-of-the-art methods by up to 13X while uncovering designs that are up to 20% better in terms of NoC. The optimized 3D manycore systems improve the EDP up to 38% when compared to 3D mesh-based designs optimized for the placement of PEs.","tags":[],"title":"MOOS: A multi-objective design space exploration and optimization framework for NoC enabled manycore systems","type":"publication"},{"authors":["Aryan Deshwal","Janardhan Rao Doppa","Dan Roth"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600744766,"objectID":"2abaf7694b4cbf1967451ac686ce8a17","permalink":"https://aryandeshwal.github.io/publication/deshwal-2019-learning/","publishdate":"2020-09-22T03:19:26.612542Z","relpermalink":"/publication/deshwal-2019-learning/","section":"publication","summary":"In a structured prediction problem, one needs to learn a predictor that, given a structured input, produces a structured object, such as a sequence, tree, or clustering output. Prototypical structured prediction tasks include part-of-speech tagging (predicting POS tag sequence for an input sentence) and semantic segmentation of images (predicting semantic labels for pixels of an input image). Unlike simple classification problems, here there is a need to assign values to multiple output variables accounting for the dependencies between them. Consequently, the prediction step itself (aka “inference” or “decoding”) is computationally-expensive, and so is the learning process, that typically requires making predictions as part of it. The key learning and inference challenge is due to the exponential size of the structured output space and depend on its complexity. In this paper, we present a unifying perspective of the different frameworks that address structured prediction problems and compare them in terms of their strengths and weaknesses. We also discuss important research directions including integration of deep learning advances into structured prediction methods, and learning from weakly supervised signals and active querying to overcome the challenges of building structured predictors from small amount of labeled data.","tags":[],"title":"Learning and inference for structured prediction: a unifying perspective","type":"publication"},{"authors":["Chao Ma","FA Rezaur Rahman Chowdhury","Aryan Deshwal","Md Rakibul Islam","Janardhan Rao Doppa","Dan Roth"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600744767,"objectID":"612ea79ca4b8ed761a5a0042b3e71ece","permalink":"https://aryandeshwal.github.io/publication/ma-2019-randomized/","publishdate":"2020-09-22T03:19:27.20383Z","relpermalink":"/publication/ma-2019-randomized/","section":"publication","summary":"In a structured prediction problem, we need to learn a predictor that can produce a structured output given a structured input (eg, part-of-speech tagging). The key learning and inference challenge is due to the exponential size of the structured output space. This paper makes four contributions towards the goal of a computationally-efficient inference and training approach for structured prediction that allows to employ complex models and to optimize for non-decomposable loss functions. First, we define a simple class of randomized greedy search (RGS) based inference procedures that leverage classification algorithms for simple outputs. Second, we develop a RGS specific learning approach for amortized inference that can quickly produce high-quality outputs for a given set of structured inputs. Third, we plug our amortized RGS inference solver inside the inner loop of parameterlearning algorithms (eg, structured SVM) to improve the speed of training. Fourth, we perform extensive experiments on diverse structured prediction tasks. Results show that our proposed approach is competitive or better than many state-ofthe-art approaches in spite of its simplicity.","tags":[],"title":"Randomized greedy search for structured prediction: amortized inference and learning","type":"publication"},{"authors":["Paul Bogdan","Fan Chen","Aryan Deshwal","Janardhan Rao Doppa","Biresh Kumar Joardar","Hai Li","Shahin Nazarian","Linghao Song","Yao Xiao"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600744767,"objectID":"9244c2caba975b4dc62f834204818552","permalink":"https://aryandeshwal.github.io/publication/bogdan-2019-taming/","publishdate":"2020-09-22T03:19:27.804426Z","relpermalink":"/publication/bogdan-2019-taming/","section":"publication","summary":"To avoid rewriting software code for new computer architectures and to take advantage of the extreme heterogeneous processing, communication and storage technologies, there is an urgent need for determining the right amount and type of specialization while making a heterogeneous system as programmable and flexible as possible. To enable both programmability and flexibility in the heterogeneous computing era, we propose a novel complex network inspired model of computation and efficient optimization algorithms for determining the optimal degree of parallelization from old software code. This mathematical framework allows us to determine the required number and type of processing elements, the amount and type of deep memory hierarchy, and the degree of reconfiguration for the communication infrastructure, thus opening new avenues to performance and energy efficiency. Our framework enables heterogeneous manycore systems to autonomously adapt from traditional switching techniques to network coding strategies in order to sustain on-chip communication in the order of terabytes. While this new programming model enables the design of self-programmable autonomous heterogeneous manycore systems, a number of open challenges will be discussed.","tags":[],"title":"Taming extreme heterogeneity via machine learning based design of autonomous manycore systems","type":"publication"}]